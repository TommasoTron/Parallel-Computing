{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TommasoTron/Parallel-Computing/blob/main/Main_PC_Challenge_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Challenge 1**"
      ],
      "metadata": {
        "id": "xvdtBiPrGllf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a **batched arbitrarily-size matrix multiplication** kernel.\n",
        "\n",
        "Let dimensions be identical for all matrix multiplications in the batch.<br>\n",
        "These being $m, k, n \\in \\mathbb{N}$.<br>\n",
        "While $batch \\in \\mathbb{N}$ is the batch size.\n",
        "\n",
        "You are provided as input the following matrices:<br>\n",
        "$N_0, N_1, N_2, ... N_{batch - 1} \\in \\mathbb{M}^{k \\times n}$<br>\n",
        "$M \\in \\mathbb{M}^{m \\times k}$\n",
        "\n",
        "You need to compute:<br>\n",
        "$P_0, P_1, P_2, ... P_{batch - 1} \\in \\mathbb{M}^{m \\times n}$\n",
        "\n",
        "Where $P_i = M \\otimes N_i$ for each $i \\in \\{0, ..., batch - 1\\}$.\n",
        "\n",
        "---\n",
        "\n",
        "A baseline reference implementation is given. Implement your version to replace it. Rely on the provided host-side function to check the correctness of results.\n",
        "\n",
        "To get a general performance metric rely on the profiler, specifically look for the `cuda_gpu_kern_sum` and try to minimize the `Total Time (ns)` of your kernel. Meanwhile, you may also want to improve `cuda_gpu_mem_time_sum`.\n",
        "\n",
        "Step one is beating the reference implementation, that should be easy, then you can use all tricks in the book to push it further.\n",
        "Anything goes, but if you use \"exotic\" tricks we want an explanation.\n",
        "In fact, submitting your work, be sure to fill out the [report](#report) with brief insights of what you did."
      ],
      "metadata": {
        "id": "xcDOQnQMGwLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General rules and advice:\n",
        "- groups are of 3 members at most, that should, for as much as possible, equally contribute to the project.\n",
        "- you automagically get ~3 points in the 1st part of the exam, taking the form of 1-2 questions that you will be allowed to skip.\n",
        "- deadline is 1 week (as of this writing, you will need to submit your work before october 23 at 23:59).\n",
        "- submissions are to be made on WeBeep, where you need to upload a downloaded copy of this notebook (.ipynb file); for groups of multiple people, it's enough for one member to submit the file in the assignment, other members shall simply write their group's name in their submission, we will then infer groups from what you write in the report section.\n",
        "- your code needs to work here on Colab with the T4 runtime.\n",
        "- do not alter code sections delimited by \"===\"s in the final submission.\n",
        "- we will change around matrix sizes arbitrarily while evaluating your work, so make sure to cover all edge cases and take care that your code is scalable (e.g. execution time grows as expected when doubling all dimensions).\n",
        "- you can get the maximum grade just by using what was discussed during lectures or is present in the glossary shown during exercise sessions; still, if you wanna have \"more fun\" this guide is your best friend https://docs.nvidia.com/cuda/cuda-c-best-practices-guide.\n",
        "- a piece of code that works is better than a supposedly faster piece of code that doesn't, so don't go overboard, but be ambitious.\n",
        "- use LLMs (ChatGPT and friends) responsibly; the purpose of this challenge is for you to get your hands dirty and build up confidence in writing parallel code through trial and error. Having an LLM write your code may get you the challenge's points (unless it's so blatant that we notice), but won't lead you to learn anything and the next time you see some parallel code your mind goes blank. If you wack your head at the problem instead, and solve it, the solution will stick in the back of your mind for a long time. Similarly, if despite pushing yourself you can't find \"that damn bug\", then asking an LLM is fine, so long as you tried first by yourself and just say \"ahhhhhh, so that what it was!\" upon having the LLM help you out. Long story short, AI is fine so long as it's a tool you **learn from** and **not** one you **blindly lean on**.\n",
        "\n",
        "If you need help or anything, please drop us an email:\n",
        "- Dr. M. Ronzani: marco.ronzani@polimi.it\n",
        "- Prof. F. Ferrandi: fabrizio.ferrandi@polimi.it"
      ],
      "metadata": {
        "id": "t0olg8POKfsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Colab Setup**"
      ],
      "metadata": {
        "id": "iHGYx97cJqaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "id": "MWrw0NgzGlMq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ITAYKD7MGcmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e613b5c-e555-4ec4-d3c2-82132d3bfa3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/cuda\n"
          ]
        }
      ],
      "source": [
        "!mkdir /home/cuda\n",
        "%cd /home/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code**"
      ],
      "metadata": {
        "id": "F9rbKG58Jyw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define tile_dim 16\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < m && col < n) {\n",
        "    for (int b = 0; b < batch; b++) {\n",
        "      float value = 0.0f;\n",
        "      for (int i = 0; i < k; i++) {\n",
        "        float a = M[row*k + i];\n",
        "        float c = N[b*(k*n) + i*n + col];\n",
        "        value += a * c;\n",
        "      }\n",
        "      P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__\n",
        "void TiledMatMul(float *M, float *N, float *P, int m, int k, int n, int batch){\n",
        "\n",
        "__shared__ float M_s[tile_dim][tile_dim];\n",
        "__shared__ float N_s[tile_dim][tile_dim];\n",
        "\n",
        "int row=blockIdx.y*tile_dim+threadIdx.y;\n",
        "int col=blockIdx.x*tile_dim+threadIdx.x;\n",
        "\n",
        "\n",
        "for(int b=0; b<batch; b++){\n",
        "  float pluto=0.0f;\n",
        "\n",
        "  // Loop over tiles\n",
        "  for (int t=0; t< (k+tile_dim-1)/tile_dim; t++){\n",
        "    // Loading tile of M into shared memory\n",
        "    if(row < m && ((t*tile_dim+threadIdx.x) < k))\n",
        "      M_s[threadIdx.y][threadIdx.x]=M[row*k + t*tile_dim + threadIdx.x];\n",
        "    else\n",
        "      M_s[threadIdx.y][threadIdx.x]=0.0f;\n",
        "\n",
        "\n",
        "    if(col < n && ((t*tile_dim+threadIdx.y) < k))\n",
        "      N_s[threadIdx.y][threadIdx.x]=N[b*(k*n)+(t*tile_dim + threadIdx.y)*n+col];\n",
        "    else\n",
        "      N_s[threadIdx.y][threadIdx.x]=0.0f;\n",
        "\n",
        " __syncthreads();\n",
        "\n",
        "    //tile multiplication\n",
        "    for(int j=0; j<tile_dim; j++){\n",
        "      pluto+=M_s[threadIdx.y][j]*N_s[j][threadIdx.x];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // Write result to global memory\n",
        "  if (row < m && col < n)\n",
        "    P[b*(m*n)+row*n+col]=pluto;\n",
        "}\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "//provato con cudamallocmanaged ma ottenevo prestazioni peggiori rispetto al solo cudamalloc\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "//  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\n",
        "  dim3 blockSize(tile_dim, tile_dim); // is 16x16 truly the best here?\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "//  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        " TiledMatMul<<<numBlocks,blockSize>>> (M_d,N_d,P_d,m,k,n,batch); // Launched TiledMatMul\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "6Ys4rptyJ5EJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e0cb51-200e-46e5-d93c-c59d09d49858"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compile, Run, Profile**"
      ],
      "metadata": {
        "id": "t4eaFcePJ_8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "gXV_BmFYKM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "PKs3tgz6KDYD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "TlgkX-SPZhqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "513bed6a-932e-4acf-a0a0-e83c3c9c3bcf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profile:"
      ],
      "metadata": {
        "id": "SBn4aD0gKRq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "VRLC6R5AKRTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d632c347-8671-4fc5-8ef2-2a8e0ea972e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-4f71.qdstrm'\n",
            "[1/8] [========================100%] report3.nsys-rep\n",
            "[2/8] [========================100%] report3.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report3.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     98.7    4,512,675,095         54  83,568,057.3  100,144,004.0     1,832  304,459,432  49,492,544.6  poll                  \n",
            "      1.2       56,436,683        543     103,935.0       12,453.0       386   17,622,531     822,627.0  ioctl                 \n",
            "      0.0        1,903,135         31      61,391.5       11,619.0     9,572    1,212,989     214,531.8  mmap64                \n",
            "      0.0          505,851          1     505,851.0      505,851.0   505,851      505,851           0.0  pthread_cond_wait     \n",
            "      0.0          484,315         10      48,431.5       59,317.0    13,202       69,139      24,222.4  sem_timedwait         \n",
            "      0.0          386,546         49       7,888.7        7,463.0     1,829       14,185       2,426.6  open64                \n",
            "      0.0          275,342         40       6,883.6        3,626.5     1,471       42,394       8,149.0  fopen                 \n",
            "      0.0          179,604         15      11,973.6        6,828.0     2,049       73,326      17,606.3  mmap                  \n",
            "      0.0          117,310          2      58,655.0       58,655.0    44,956       72,354      19,373.3  pthread_create        \n",
            "      0.0           70,586         12       5,882.2        6,051.5     1,198       13,026       3,510.1  write                 \n",
            "      0.0           51,317         33       1,555.1        1,049.0       742        6,547       1,212.5  fclose                \n",
            "      0.0           48,244         20       2,412.2           45.5        45       47,269      10,558.2  fgets                 \n",
            "      0.0           32,037         64         500.6          534.0       162        1,101         189.1  fcntl                 \n",
            "      0.0           29,876          6       4,979.3        4,794.0     1,769        8,208       2,620.5  open                  \n",
            "      0.0           27,761          5       5,552.2        5,080.0     4,030        8,671       1,808.1  munmap                \n",
            "      0.0           25,443          2      12,721.5       12,721.5     8,119       17,324       6,508.9  socket                \n",
            "      0.0           22,623         15       1,508.2        1,438.0       519        3,781         872.7  read                  \n",
            "      0.0           19,535          3       6,511.7        5,950.0     4,312        9,273       2,527.7  pipe2                 \n",
            "      0.0           11,297          1      11,297.0       11,297.0    11,297       11,297           0.0  connect               \n",
            "      0.0            6,996          2       3,498.0        3,498.0     2,145        4,851       1,913.4  pthread_cond_broadcast\n",
            "      0.0            5,895          2       2,947.5        2,947.5     2,617        3,278         467.4  fwrite                \n",
            "      0.0            3,025          8         378.1          344.0       303          627         105.0  dup                   \n",
            "      0.0            1,840          1       1,840.0        1,840.0     1,840        1,840           0.0  bind                  \n",
            "      0.0              920          1         920.0          920.0       920          920           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     89.0       81,511,148          3  27,170,382.7     78,868.0     69,660  81,362,620  46,931,854.4  cudaMalloc            \n",
            "      5.1        4,670,172          1   4,670,172.0  4,670,172.0  4,670,172   4,670,172           0.0  cudaDeviceSynchronize \n",
            "      4.7        4,313,892          3   1,437,964.0  1,432,307.0    130,504   2,751,081   1,310,297.7  cudaMemcpy            \n",
            "      1.0          920,525          3     306,841.7    223,394.0    135,964     561,167     224,548.5  cudaFree              \n",
            "      0.2          168,630          1     168,630.0    168,630.0    168,630     168,630           0.0  cudaLaunchKernel      \n",
            "      0.0            1,815          1       1,815.0      1,815.0      1,815       1,815           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                             Name                           \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------\n",
            "    100.0        4,665,858          1  4,665,858.0  4,665,858.0  4,665,858  4,665,858          0.0  TiledMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     70.9        2,595,212      2  1,297,606.0  1,297,606.0     45,631  2,549,581  1,770,560.0  [CUDA memcpy HtoD]\n",
            "     29.1        1,063,211      1  1,063,211.0  1,063,211.0  1,063,211  1,063,211          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     13.107      2     6.554     6.554     0.524    12.583        8.527  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report3.nsys-rep\n",
            "    /home/cuda/report3.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"report\"></a>\n",
        "## **Brief Report**"
      ],
      "metadata": {
        "id": "edxNOq-8PCdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You must fill in this section!!**\n",
        "\n",
        "Group information:\n",
        "- member-1: NAME, SURNAME, PERSON CODE\n",
        "- member-2: NAME, SURNAME, PERSON CODE\n",
        "- member-3: NAME, SURNAME, PERSON CODE\n",
        "- your group's NAME and LOGO<br><img src=\"https://static.wikia.nocookie.net/86-eighty-six/images/d/dc/Undertaker_emblem.png/revision/latest?cb=20210311091258\" alt=\"+0.005 points at the exam if you know the reference ^-^\" width=\"120\" border=\"0\">\n",
        "\n",
        "*Note: yes, groups can now have a logo - this is optional and merely for fun, if you don't feel like having one, no worries, in which case you may delete that itemize entry alongside this note :(*\n",
        "<!-- if you reeeeeally don't have ideas for a logo, before giving up, check this out: https://picrew.me/en/image_maker/47882 -->"
      ],
      "metadata": {
        "id": "5COx5mKMPF7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bullet points describing what you did with a short motivation (some - arguably stupid ? - examples are given):\n",
        "- used supershared L99 cache: this was the fastest way to raise the temperature and cook an egg on the GPU's heatsink\n",
        "- pinned DRAM chips to the wall and asked them to be faster: they did not comply\n",
        "- missread the assignment and implemented matrix diagonalization: now I have my own version of cuBLAS\n",
        "- relied so heavily on blockIdx.z that the results tried to escape the HBM stack: we politely asked them to stay\n",
        "- broke isolation and achieved priviledge escalation in Colab by kindly asking Google's chief sysadmin, we can now \"tweak\" the profiler's report: this was outside what was discussed during lectures, but social engineering is easier than writing good code\n",
        "\n",
        "*Note: possibly less than 8 entries of ~32 words each. More isn't necessarily better if nobody will read it.*\n",
        "\n",
        "*Note: the subject is \"the main things you came up with to improve the kernel\".*"
      ],
      "metadata": {
        "id": "PRKJGx2MR7LQ"
      }
    }
  ]
}