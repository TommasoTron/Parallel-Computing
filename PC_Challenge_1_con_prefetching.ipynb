{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TommasoTron/Parallel-Computing/blob/main/PC_Challenge_1_con_prefetching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Challenge 1**"
      ],
      "metadata": {
        "id": "xvdtBiPrGllf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a **batched arbitrarily-size matrix multiplication** kernel.\n",
        "\n",
        "Let dimensions be identical for all matrix multiplications in the batch.<br>\n",
        "These being $m, k, n \\in \\mathbb{N}$.<br>\n",
        "While $batch \\in \\mathbb{N}$ is the batch size.\n",
        "\n",
        "You are provided as input the following matrices:<br>\n",
        "$N_0, N_1, N_2, ... N_{batch - 1} \\in \\mathbb{M}^{k \\times n}$<br>\n",
        "$M \\in \\mathbb{M}^{m \\times k}$\n",
        "\n",
        "You need to compute:<br>\n",
        "$P_0, P_1, P_2, ... P_{batch - 1} \\in \\mathbb{M}^{m \\times n}$\n",
        "\n",
        "Where $P_i = M \\otimes N_i$ for each $i \\in \\{0, ..., batch - 1\\}$.\n",
        "\n",
        "---\n",
        "\n",
        "A baseline reference implementation is given. Implement your version to replace it. Rely on the provided host-side function to check the correctness of results.\n",
        "\n",
        "To get a general performance metric rely on the profiler, specifically look for the `cuda_gpu_kern_sum` and try to minimize the `Total Time (ns)` of your kernel. Meanwhile, you may also want to improve `cuda_gpu_mem_time_sum`.\n",
        "\n",
        "Step one is beating the reference implementation, that should be easy, then you can use all tricks in the book to push it further.\n",
        "Anything goes, but if you use \"exotic\" tricks we want an explanation.\n",
        "In fact, submitting your work, be sure to fill out the [report](#report) with brief insights of what you did."
      ],
      "metadata": {
        "id": "xcDOQnQMGwLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General rules and advice:\n",
        "- groups are of 3 members at most, that should, for as much as possible, equally contribute to the project.\n",
        "- you automagically get ~3 points in the 1st part of the exam, taking the form of 1-2 questions that you will be allowed to skip.\n",
        "- deadline is 1 week (as of this writing, you will need to submit your work before october 23 at 23:59).\n",
        "- submissions are to be made on WeBeep, where you need to upload a downloaded copy of this notebook (.ipynb file); for groups of multiple people, it's enough for one member to submit the file in the assignment, other members shall simply write their group's name in their submission, we will then infer groups from what you write in the report section.\n",
        "- your code needs to work here on Colab with the T4 runtime.\n",
        "- do not alter code sections delimited by \"===\"s in the final submission.\n",
        "- we will change around matrix sizes arbitrarily while evaluating your work, so make sure to cover all edge cases and take care that your code is scalable (e.g. execution time grows as expected when doubling all dimensions).\n",
        "- you can get the maximum grade just by using what was discussed during lectures or is present in the glossary shown during exercise sessions; still, if you wanna have \"more fun\" this guide is your best friend https://docs.nvidia.com/cuda/cuda-c-best-practices-guide.\n",
        "- a piece of code that works is better than a supposedly faster piece of code that doesn't, so don't go overboard, but be ambitious.\n",
        "- use LLMs (ChatGPT and friends) responsibly; the purpose of this challenge is for you to get your hands dirty and build up confidence in writing parallel code through trial and error. Having an LLM write your code may get you the challenge's points (unless it's so blatant that we notice), but won't lead you to learn anything and the next time you see some parallel code your mind goes blank. If you wack your head at the problem instead, and solve it, the solution will stick in the back of your mind for a long time. Similarly, if despite pushing yourself you can't find \"that damn bug\", then asking an LLM is fine, so long as you tried first by yourself and just say \"ahhhhhh, so that what it was!\" upon having the LLM help you out. Long story short, AI is fine so long as it's a tool you **learn from** and **not** one you **blindly lean on**.\n",
        "\n",
        "If you need help or anything, please drop us an email:\n",
        "- Dr. M. Ronzani: marco.ronzani@polimi.it\n",
        "- Prof. F. Ferrandi: fabrizio.ferrandi@polimi.it"
      ],
      "metadata": {
        "id": "t0olg8POKfsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Colab Setup**"
      ],
      "metadata": {
        "id": "iHGYx97cJqaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "id": "MWrw0NgzGlMq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ITAYKD7MGcmH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e613b5c-e555-4ec4-d3c2-82132d3bfa3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/home/cuda\n"
          ]
        }
      ],
      "source": [
        "!mkdir /home/cuda\n",
        "%cd /home/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code**"
      ],
      "metadata": {
        "id": "F9rbKG58Jyw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "#define tile_dim 32\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "// this is the reference implementation\n",
        "// you can change this to your heart's contempt\n",
        "\n",
        "__global__\n",
        "void batchedMatMul(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  int row = blockIdx.y*blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "  if (row < m && col < n) {\n",
        "    for (int b = 0; b < batch; b++) {\n",
        "      float value = 0.0f;\n",
        "      for (int i = 0; i < k; i++) {\n",
        "        float a = M[row*k + i];\n",
        "        float c = N[b*(k*n) + i*n + col];\n",
        "        value += a * c;\n",
        "      }\n",
        "      P[b*(m*n) + row*n + col] = value;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "__global__\n",
        "void TiledMatMul(float *M, float *N, float *P, int m, int k, int n, int batch){\n",
        "\n",
        "__shared__ float M_s[2][tile_dim][tile_dim];\n",
        "__shared__ float N_s[2][tile_dim][tile_dim];\n",
        "\n",
        "int row=blockIdx.y*tile_dim+threadIdx.y;\n",
        "int col=blockIdx.x*tile_dim+threadIdx.x;\n",
        "\n",
        "int numTiles= (k+tile_dim-1)/tile_dim;\n",
        "\n",
        "for(int b=0; b<batch; b++){\n",
        "  float pluto=0.0f;\n",
        "  int current=0;\n",
        "int t=0;\n",
        "//proviamo a fare sto prefetch della prima tile\n",
        "    if(row < m && ((t*tile_dim+threadIdx.x) < k))\n",
        "      M_s[current][threadIdx.y][threadIdx.x]=M[row*k + t*tile_dim + threadIdx.x];\n",
        "    else\n",
        "      M_s[current][threadIdx.y][threadIdx.x]=0.0f;\n",
        "\n",
        "\n",
        "    if(col < n && ((t*tile_dim+threadIdx.y) < k))\n",
        "      N_s[current][threadIdx.y][threadIdx.x]=N[b*(k*n)+(t*tile_dim + threadIdx.y)*n+col];\n",
        "    else\n",
        "      N_s[current][threadIdx.y][threadIdx.x]=0.0f;\n",
        "\n",
        "__syncthreads();\n",
        "\n",
        "\n",
        "  // Loop over tiles\n",
        "  for (t=0; t< numTiles; t++){\n",
        "int quello_dopo=1-current;\n",
        "\n",
        "\n",
        "if(t+1<numTiles){\n",
        "    if(row < m && ((t*tile_dim+threadIdx.x) < k))\n",
        "      M_s[quello_dopo][threadIdx.y][threadIdx.x]=M[row*k + (1+t)*tile_dim + threadIdx.x];\n",
        "    else\n",
        "      M_s[quello_dopo][threadIdx.y][threadIdx.x]=0.0f;\n",
        "\n",
        "\n",
        "    if(col < n && ((t*tile_dim+threadIdx.y) < k))\n",
        "      N_s[quello_dopo][threadIdx.y][threadIdx.x]=N[b*(k*n)+((t+1)*tile_dim + threadIdx.y)*n+col];\n",
        "    else\n",
        "      N_s[quello_dopo][threadIdx.y][threadIdx.x]=0.0f;\n",
        "}\n",
        "// __syncthreads();\n",
        "\n",
        "    //multiplication\n",
        "    for(int j=0; j<tile_dim; j++){\n",
        "      pluto+=M_s[current][threadIdx.y][j]*N_s[current][j][threadIdx.x];\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "    current=quello_dopo;\n",
        "  }\n",
        "\n",
        "  // Write result to global memory\n",
        "  if (row < m && col < n)\n",
        "    P[b*(m*n)+row*n+col]=pluto;\n",
        "}\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "\n",
        "  float* M = (float*)malloc(sizeM * sizeof(float));\n",
        "  float* N = (float*)malloc(sizeN * sizeof(float));\n",
        "  float* P = (float*)malloc(sizeP * sizeof(float));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "//provato con cudamallocmanaged ma ottenevo prestazioni peggiori rispetto al solo cudamalloc\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "//  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "\n",
        "  dim3 blockSize(tile_dim, tile_dim); // is 16x16 truly the best here?\n",
        "  dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y);\n",
        "\n",
        "//  batchedMatMul<<<numBlocks, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "\n",
        " TiledMatMul<<<numBlocks,blockSize>>> (M_d,N_d,P_d,m,k,n,batch); // Launched TiledMatMul\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  free(M);\n",
        "  free(N);\n",
        "  free(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "6Ys4rptyJ5EJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04da2548-a6d1-4978-8a34-3b162c6e426e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bmatmul.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compile, Run, Profile**"
      ],
      "metadata": {
        "id": "t4eaFcePJ_8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "gXV_BmFYKM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "PKs3tgz6KDYD"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "TlgkX-SPZhqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc8bbe9f-18ee-4e9c-9ff8-1300c10d61d2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profile:"
      ],
      "metadata": {
        "id": "SBn4aD0gKRq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "VRLC6R5AKRTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cca69d9-ba71-4549-9c2c-6806529ba1bf"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running batched matmul with m=256, k=512, n=384, batch=16, seed=119\n",
            "Checking results on CPU...\n",
            "All results matched, success!Generating '/tmp/nsys-report-d35b.qdstrm'\n",
            "[1/8] [========================100%] report8.nsys-rep\n",
            "[2/8] [========================100%] report8.sqlite\n",
            "[3/8] Executing 'nvtx_sum' stats report\n",
            "SKIPPED: /home/cuda/report8.sqlite does not contain NV Tools Extension (NVTX) data.\n",
            "[4/8] Executing 'osrt_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)   Max (ns)    StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -------------  --------  -----------  ------------  ----------------------\n",
            "     98.5    4,503,411,314         54  83,396,505.8  100,138,914.0     1,954  296,805,871  48,881,051.4  poll                  \n",
            "      1.4       63,885,885        543     117,653.6       11,727.0       392   17,292,060     940,507.0  ioctl                 \n",
            "      0.0        1,911,619         31      61,665.1       13,532.0     9,016    1,221,892     216,089.6  mmap64                \n",
            "      0.0          739,606         10      73,960.6       55,429.5    26,139      270,595      69,955.0  sem_timedwait         \n",
            "      0.0          487,040          1     487,040.0      487,040.0   487,040      487,040           0.0  pthread_cond_wait     \n",
            "      0.0          422,709         49       8,626.7        7,386.0     3,300       27,039       3,901.6  open64                \n",
            "      0.0          226,558         40       5,664.0        3,535.0     1,240       32,416       6,328.2  fopen                 \n",
            "      0.0          183,731         15      12,248.7        7,516.0     1,467       61,065      14,772.5  mmap                  \n",
            "      0.0          105,115          2      52,557.5       52,557.5    39,916       65,199      17,877.8  pthread_create        \n",
            "      0.0           63,510         12       5,292.5        5,349.0     1,404        7,830       1,769.0  write                 \n",
            "      0.0           49,100         33       1,487.9        1,239.0       769        5,514       1,075.4  fclose                \n",
            "      0.0           36,757         20       1,837.9           45.0        43       35,780       7,989.2  fgets                 \n",
            "      0.0           33,261          6       5,543.5        4,357.0     1,433       11,025       3,579.3  open                  \n",
            "      0.0           32,939         64         514.7          524.0       179        1,150         208.8  fcntl                 \n",
            "      0.0           27,383         15       1,825.5        1,569.0       865        4,612         961.6  read                  \n",
            "      0.0           26,727          5       5,345.4        4,781.0     3,468        7,193       1,528.4  munmap                \n",
            "      0.0           22,893          2      11,446.5       11,446.5     6,112       16,781       7,544.1  socket                \n",
            "      0.0           17,855          3       5,951.7        6,195.0     2,930        8,730       2,907.6  pipe2                 \n",
            "      0.0           10,488          1      10,488.0       10,488.0    10,488       10,488           0.0  connect               \n",
            "      0.0            7,370          2       3,685.0        3,685.0     2,434        4,936       1,769.2  pthread_cond_broadcast\n",
            "      0.0            6,064          2       3,032.0        3,032.0     2,740        3,324         413.0  fwrite                \n",
            "      0.0            3,308          8         413.5          340.5       255          817         181.2  dup                   \n",
            "      0.0            1,657          1       1,657.0        1,657.0     1,657        1,657           0.0  bind                  \n",
            "      0.0            1,046          1       1,046.0        1,046.0     1,046        1,046           0.0  listen                \n",
            "\n",
            "[5/8] Executing 'cuda_api_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Num Calls    Avg (ns)     Med (ns)    Min (ns)    Max (ns)   StdDev (ns)            Name         \n",
            " --------  ---------------  ---------  ------------  -----------  ---------  ----------  ------------  ----------------------\n",
            "     89.7       89,297,544          3  29,765,848.0     83,163.0     82,546  89,131,835  51,412,452.9  cudaMalloc            \n",
            "      4.9        4,886,117          1   4,886,117.0  4,886,117.0  4,886,117   4,886,117           0.0  cudaDeviceSynchronize \n",
            "      4.3        4,315,466          3   1,438,488.7  1,421,217.0    132,800   2,761,449   1,314,409.6  cudaMemcpy            \n",
            "      0.9          898,201          3     299,400.3    235,775.0    114,454     547,972     223,652.9  cudaFree              \n",
            "      0.2          152,880          1     152,880.0    152,880.0    152,880     152,880           0.0  cudaLaunchKernel      \n",
            "      0.0            1,565          1       1,565.0      1,565.0      1,565       1,565           0.0  cuModuleGetLoadingMode\n",
            "\n",
            "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)                             Name                           \n",
            " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------------------------------------------------------\n",
            "    100.0        4,882,527          1  4,882,527.0  4,882,527.0  4,882,527  4,882,527          0.0  TiledMatMul(float *, float *, float *, int, int, int, int)\n",
            "\n",
            "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
            "\n",
            " Time (%)  Total Time (ns)  Count   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)      Operation     \n",
            " --------  ---------------  -----  -----------  -----------  ---------  ---------  -----------  ------------------\n",
            "     71.2        2,599,917      2  1,299,958.5  1,299,958.5     48,127  2,551,790  1,770,357.1  [CUDA memcpy HtoD]\n",
            "     28.8        1,052,843      1  1,052,843.0  1,052,843.0  1,052,843  1,052,843          0.0  [CUDA memcpy DtoH]\n",
            "\n",
            "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
            "\n",
            " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     \n",
            " ----------  -----  --------  --------  --------  --------  -----------  ------------------\n",
            "     13.107      2     6.554     6.554     0.524    12.583        8.527  [CUDA memcpy HtoD]\n",
            "      6.291      1     6.291     6.291     6.291     6.291        0.000  [CUDA memcpy DtoH]\n",
            "\n",
            "Generated:\n",
            "    /home/cuda/report8.nsys-rep\n",
            "    /home/cuda/report8.sqlite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"report\"></a>\n",
        "## **Brief Report**"
      ],
      "metadata": {
        "id": "edxNOq-8PCdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You must fill in this section!!**\n",
        "\n",
        "Group information:\n",
        "- member-1: NAME, SURNAME, PERSON CODE\n",
        "- member-2: NAME, SURNAME, PERSON CODE\n",
        "- member-3: NAME, SURNAME, PERSON CODE\n",
        "- your group's NAME and LOGO<br><img src=\"https://static.wikia.nocookie.net/86-eighty-six/images/d/dc/Undertaker_emblem.png/revision/latest?cb=20210311091258\" alt=\"+0.005 points at the exam if you know the reference ^-^\" width=\"120\" border=\"0\">\n",
        "\n",
        "*Note: yes, groups can now have a logo - this is optional and merely for fun, if you don't feel like having one, no worries, in which case you may delete that itemize entry alongside this note :(*\n",
        "<!-- if you reeeeeally don't have ideas for a logo, before giving up, check this out: https://picrew.me/en/image_maker/47882 -->"
      ],
      "metadata": {
        "id": "5COx5mKMPF7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bullet points describing what you did with a short motivation (some - arguably stupid ? - examples are given):\n",
        "- used supershared L99 cache: this was the fastest way to raise the temperature and cook an egg on the GPU's heatsink\n",
        "- pinned DRAM chips to the wall and asked them to be faster: they did not comply\n",
        "- missread the assignment and implemented matrix diagonalization: now I have my own version of cuBLAS\n",
        "- relied so heavily on blockIdx.z that the results tried to escape the HBM stack: we politely asked them to stay\n",
        "- broke isolation and achieved priviledge escalation in Colab by kindly asking Google's chief sysadmin, we can now \"tweak\" the profiler's report: this was outside what was discussed during lectures, but social engineering is easier than writing good code\n",
        "\n",
        "*Note: possibly less than 8 entries of ~32 words each. More isn't necessarily better if nobody will read it.*\n",
        "\n",
        "*Note: the subject is \"the main things you came up with to improve the kernel\".*"
      ],
      "metadata": {
        "id": "PRKJGx2MR7LQ"
      }
    }
  ]
}